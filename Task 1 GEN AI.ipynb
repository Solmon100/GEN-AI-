{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a367e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46c9dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MajorClaim</td>\n",
       "      <td>we should attach more importance to cooperatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Premise</td>\n",
       "      <td>Take Olympic games which is a form of competit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Premise</td>\n",
       "      <td>The high technology and new ideas applied into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Premise</td>\n",
       "      <td>pollutions are not just caused by the burning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Premise</td>\n",
       "      <td>the improvements of work efficiency also attri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6084</th>\n",
       "      <td>MajorClaim</td>\n",
       "      <td>addressing pollution and traffic issues only b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>Premise</td>\n",
       "      <td>whether it can work out for alleviating traffi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>Premise</td>\n",
       "      <td>price control institution has been used in ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>Claim</td>\n",
       "      <td>it seems not easy to increase petrol price ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>Premise</td>\n",
       "      <td>governments have a macro-economic perspective ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6089 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                               text\n",
       "0     MajorClaim  we should attach more importance to cooperatio...\n",
       "1        Premise  Take Olympic games which is a form of competit...\n",
       "2        Premise  The high technology and new ideas applied into...\n",
       "3        Premise  pollutions are not just caused by the burning ...\n",
       "4        Premise  the improvements of work efficiency also attri...\n",
       "...          ...                                                ...\n",
       "6084  MajorClaim  addressing pollution and traffic issues only b...\n",
       "6085     Premise  whether it can work out for alleviating traffi...\n",
       "6086     Premise  price control institution has been used in ple...\n",
       "6087       Claim  it seems not easy to increase petrol price ins...\n",
       "6088     Premise  governments have a macro-economic perspective ...\n",
       "\n",
       "[6089 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the directory containing the text files\n",
    "directory = r\"D:\\\\assigments innomatics\\\\GEN AI\\\\text_data\\\\text\"\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            label = lines[0].strip()  # Assume the first line is the label\n",
    "            text = ' '.join(line.strip() for line in lines[1:])  # Rest of the lines are the text\n",
    "            data.append((label, text))\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df = pd.DataFrame(data, columns=['label', 'text'])\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0679859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MajorClaim</td>\n",
       "      <td>we should attach more importance to cooperatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Premise</td>\n",
       "      <td>Take Olympic games which is a form of competit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Premise</td>\n",
       "      <td>The high technology and new ideas applied into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Premise</td>\n",
       "      <td>pollutions are not just caused by the burning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Premise</td>\n",
       "      <td>the improvements of work efficiency also attri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text\n",
       "0  MajorClaim  we should attach more importance to cooperatio...\n",
       "1     Premise  Take Olympic games which is a form of competit...\n",
       "2     Premise  The high technology and new ideas applied into...\n",
       "3     Premise  pollutions are not just caused by the burning ...\n",
       "4     Premise  the improvements of work efficiency also attri..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a133173d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6084</th>\n",
       "      <td>MajorClaim</td>\n",
       "      <td>addressing pollution and traffic issues only b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>Premise</td>\n",
       "      <td>whether it can work out for alleviating traffi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>Premise</td>\n",
       "      <td>price control institution has been used in ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>Claim</td>\n",
       "      <td>it seems not easy to increase petrol price ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>Premise</td>\n",
       "      <td>governments have a macro-economic perspective ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                               text\n",
       "6084  MajorClaim  addressing pollution and traffic issues only b...\n",
       "6085     Premise  whether it can work out for alleviating traffi...\n",
       "6086     Premise  price control institution has been used in ple...\n",
       "6087       Claim  it seems not easy to increase petrol price ins...\n",
       "6088     Premise  governments have a macro-economic perspective ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315cf1c9",
   "metadata": {},
   "source": [
    "### In what format are the observations available in the given raw data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38addc01",
   "metadata": {},
   "source": [
    "#### 1. Ans --- Text format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4e596",
   "metadata": {},
   "source": [
    "### In what format are the observations available in the given raw data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04329d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6089, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05833e",
   "metadata": {},
   "source": [
    "### How many files are having 'Premise' label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d15fc8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3832"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()[\"Premise\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d04ac6",
   "metadata": {},
   "source": [
    "### What is the maximum number of character level tokens in a document in raw 'text' column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "668dd615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokens(text):\n",
    "    return list(text)\n",
    "\n",
    "df['num_chars'] = df['text'].apply(lambda x: len(char_tokens(x)))\n",
    "max_chars = df['num_chars'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4af4bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d579b8a9",
   "metadata": {},
   "source": [
    "## What is the maximum number of word level tokens in a document in raw 'text' columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1e745b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"num_words\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "max_words = df['num_words'].max()\n",
    "max_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563b7c1",
   "metadata": {},
   "source": [
    "## After applying text cleaning (i.e. text pre-processing as mentioned below), what is the maximum number of word level tokens in 'clean_text' column?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1d78d",
   "metadata": {},
   "source": [
    "### Cleaning Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd031774",
   "metadata": {},
   "source": [
    "1. A valid token should only contain alphanumeric and \".\"\n",
    "2. Convert to lower\n",
    "3. word tokenize\n",
    "4. stop word removal\n",
    "5. lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a38aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5cfff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\solmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\solmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\solmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c29b7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of stop words: \n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"list of stop words: \")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1937a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    #match digits and special characters\n",
    "    pattern = r\"[^a-zA-Z.]\"\n",
    "    \n",
    "    text = re.sub(pattern,\" \",text)\n",
    "    \n",
    "    #convert lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    #word tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filltered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lem_tokens = [lemmatizer.lemmatize(token) for token in filltered_tokens]\n",
    "    \n",
    "    return \" \".join(lem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4b0837b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_words</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MajorClaim</td>\n",
       "      <td>we should attach more importance to cooperatio...</td>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "      <td>attach importance cooperation primary education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Premise</td>\n",
       "      <td>Take Olympic games which is a form of competit...</td>\n",
       "      <td>297</td>\n",
       "      <td>58</td>\n",
       "      <td>take olympic game form competition instance ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Premise</td>\n",
       "      <td>The high technology and new ideas applied into...</td>\n",
       "      <td>154</td>\n",
       "      <td>26</td>\n",
       "      <td>high technology new idea applied practice may ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Premise</td>\n",
       "      <td>pollutions are not just caused by the burning ...</td>\n",
       "      <td>137</td>\n",
       "      <td>20</td>\n",
       "      <td>pollution caused burning oil chemical pollutan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Premise</td>\n",
       "      <td>the improvements of work efficiency also attri...</td>\n",
       "      <td>127</td>\n",
       "      <td>19</td>\n",
       "      <td>improvement work efficiency also attribute spe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text  num_chars  \\\n",
       "0  MajorClaim  we should attach more importance to cooperatio...         72   \n",
       "1     Premise  Take Olympic games which is a form of competit...        297   \n",
       "2     Premise  The high technology and new ideas applied into...        154   \n",
       "3     Premise  pollutions are not just caused by the burning ...        137   \n",
       "4     Premise  the improvements of work efficiency also attri...        127   \n",
       "\n",
       "   num_words                                         clean_text  \n",
       "0         10    attach importance cooperation primary education  \n",
       "1         58  take olympic game form competition instance ha...  \n",
       "2         26  high technology new idea applied practice may ...  \n",
       "3         20  pollution caused burning oil chemical pollutan...  \n",
       "4         19  improvement work efficiency also attribute spe...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].apply(lambda x: clean_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb423c0",
   "metadata": {},
   "source": [
    "## what is the maximum number of word level tokens in 'clean_text' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cb41ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_tokens(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df[\"word_tokens_text\"] = df[\"clean_text\"].apply(lambda x: len(word_tokens(x)))\n",
    "max_level_tokens = df[\"word_tokens_text\"].max()\n",
    "max_level_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a230a45",
   "metadata": {},
   "source": [
    "## What is the output if you apply all the text pre-processing given above on file_1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b917196a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attach importance cooperation primary education'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67e023f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_text\"].apply(lambda x: len(word_tokens(x)))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5351408a",
   "metadata": {},
   "source": [
    "### Apply Vectorization using Bag of Word Technique(with default parameters) on the raw text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2031e2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dm: (6089, 7052)\n",
      "unique vocablary words: ['00' '000' '10' ... 'zoo' 'zookeepers' 'zoos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_word = CountVectorizer()\n",
    "dm_text = bag_of_word.fit_transform(df[\"text\"])\n",
    "\n",
    "print(f\"shape of dm: {dm_text.shape}\")\n",
    "print(f\"unique vocablary words: {bag_of_word.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bef50a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dm: (6089, 7052)\n",
      "unique vocablary words: ['00' '000' '10' ... 'zoo' 'zookeepers' 'zoos']\n",
      "Shape of CSR matrix: (6089, 7052)\n",
      "Data array (non-zero elements): \n",
      "[1 1 1 ... 1 1 1]\n",
      "Indices array (column indices): \n",
      "[6873 5686  490 ... 4641 1631 3840]\n",
      "Indptr array (row pointers): \n",
      "[    0    10    53 ... 86857 86866 86880]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_word = CountVectorizer(ngram_range=(1, 1))\n",
    "dm_text = bag_of_word.fit_transform(df[\"text\"])\n",
    "csr_matrix = dm_text\n",
    "\n",
    "print(f\"shape of dm: {dm_text.shape}\")\n",
    "print(f\"unique vocablary words: {bag_of_word.get_feature_names_out()}\")\n",
    "print(f\"Shape of CSR matrix: {csr_matrix.shape}\")\n",
    "print(f\"Data array (non-zero elements): \\n{csr_matrix.data}\")\n",
    "print(f\"Indices array (column indices): \\n{csr_matrix.indices}\")\n",
    "print(f\"Indptr array (row pointers): \\n{csr_matrix.indptr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff877840",
   "metadata": {},
   "source": [
    "## Apply Vectorization using Bag of Word Vectorization(with following parameters) on the raw text column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f1ad6",
   "metadata": {},
   "source": [
    "1.token_pattern=None\n",
    "2.tokenizer=callable use nltk word tokenizer\n",
    "3.ngram_range=(1, **n)\n",
    "4. lowercase=false\n",
    "5. preprocessor=callable \n",
    "  5.1 A valid token should only contain alphanumeric and \".\"\n",
    "  5.2 Convert to lower\n",
    "  5.3 word tokenize\n",
    "  5.4 stop word removal\n",
    "  5.5 lemmatization\n",
    "    \n",
    "6. stop_words=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2bae61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67891bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique vocabulary: (6089, 5982)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_word = CountVectorizer(token_pattern = None,\n",
    "                                  lowercase = False,\n",
    "                                  preprocessor = clean_text,\n",
    "                                  tokenizer = word_tokenize,\n",
    "                                  stop_words = None,\n",
    "                                  ngram_range = (1,1)\n",
    "                                 ) \n",
    "text_dm = bag_of_word.fit_transform(df[\"text\"])\n",
    "print(f'unique vocabulary: {text_dm.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a27b0704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique vocabulary: (6089, 42023)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_word = CountVectorizer(token_pattern = None,\n",
    "                                  lowercase = False,\n",
    "                                  preprocessor = clean_text,\n",
    "                                  tokenizer = word_tokenize,\n",
    "                                  stop_words = None,\n",
    "                                  ngram_range = (1,2)\n",
    "                                 ) \n",
    "text_dm = bag_of_word.fit_transform(df[\"text\"])\n",
    "print(f'unique vocabulary: {text_dm.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f8444d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique vocabulary: (6089, 79862)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_word = CountVectorizer(token_pattern = None,\n",
    "                                  lowercase = False,\n",
    "                                  preprocessor = clean_text,\n",
    "                                  tokenizer = word_tokenize,\n",
    "                                  stop_words = None,\n",
    "                                  ngram_range = (1,3)\n",
    "                                 ) \n",
    "text_dm = bag_of_word.fit_transform(df[\"text\"])\n",
    "print(f'unique vocabulary: {text_dm.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae148c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
